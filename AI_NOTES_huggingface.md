# AI Notes

## Transformers

From rule based systems, to probability in statistical models, matrix embedded space. Deep learning models. Tokenizer models.

Cosine similarly math. Embedding vector trigger function math. Probability math 
Softmax. Lora onnx BPE back propagation attention. Encoder decoder. 

Multi head attention , self attention.
Sine and cosine linear functions.
Positional transform
Sine for even index
Cosine for odd index
Normalization 

NLP - human language understand 
Convert to numbers for
Computer understand 

Tokens, words, subwords, characters 
Generate math embeddings from tokens.

Vocabulary 50,000 words 

UNK - any token outside VOCABULARY 

Subword, byte pair encoding for UNK.
Character is large number tokens.

Embeddings for neural Network.

ML models only understand numbers not text.

Convert text to numbers = understanding.

Attention is - super embeddings.

Convert words, but also position, order relation of words for meaning.

Sequence to sequence and attention mech

Tokens become dense vectors.

OVERRIDE CHARACTER ENCODING.

Similar words have similar vectors.

Custom TOKENIZER types

BPE TOKENIZER 

WordPiece TOKENIZER 

SentencePiece TOKENIZER 

aSYMBOLZa TOKENIZER 

NLTK library NLP toolkit 

Many different ML architecture, bert robert CNN RNN.

Different encoding, decoding TOKENIZERS.

Metaspace TOKENIZER replaces space with underscore.

BPE TOKENIZER uses frequency of toks to determine words in TRAINING embeddings then either knows the WORDS or does not.

Need to train transformer.

PRE-TRAINED auto TOKENIZER bert gpt

BPE solves rich vocabulary problem by making patterns of the TOKENS. 

To ascribed meaning with numbers, that represent dense vectors in embed space.

Most frequent token pair up to number of tokens allowed.

WordPiece uses probability distribution not frequency.

# NLP

LLMs, with GPT and Llama, shifted task specific models to general purpose models.

Careful text representation to learn.
AMBIGUITY cliche sarcasm culture humor

Solution - hybrid language 
Optimize the input

Part-of-Speech TAGGING.

NAMED ENTITY RECOGNITION 

Billions of parameters, enables general purpose. 

Can learn from PROMPT.
In context learning.

EMERGENT ABILITIES 

Context windows.

Paradigm Shift in NLP, to single large model - not small specialized models.

Because fine tuned.

Preprocessing - computer readable 
Postprocessing - human readable

Fill mask , bert - guess the word.

Grouped_Entities - find things.
PER, ORG, LOC...

Pipeline() is wrapper for multi use 
It connect model with pre post processing.

Model is cached.

Zero shot - classify with zero prior training - probability of labels of guessed CONTEXT.

NER named entity recognition 

Hyperparams control FINE-TUNING.
Learning rate batch size epoch 




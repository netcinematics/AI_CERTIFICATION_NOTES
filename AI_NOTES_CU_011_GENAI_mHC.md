FOUNDATION (GEN AI) - FRONTIER (mHC)


FOUNDATION (GEN AI):

Inside the blackbox.

Predicting a triangle (visually) with probability between 0 / 1.

discriminative AI of many features to small features.

Prompt completion.

distribution sampling - unpredictability.

dice roll inside blackbox. Every time unique answer.

token predicts next token.

Next word score. 

### TRANSLATION - TEXT to SPEECH.

source language input

target language output

encoder - take input sequence = generate representation.

capture useful information in tokens.

pass tokens to decoder,

input sentence to many tokens.

Traditional RNN.

Attention mechanism allows encoder to pay more 

attention to every token - not just one.

Output - generate waveform (audio token).

Generator Network. 1 token is 3ms. = 4 token 12ms.

4 - 3 - 1 normalized. softmax 0 - 1.

Trace Precedence.

convolution network - transform text to audio 

token to 3ms. 

Generative transform from text to audio.

------------------------------

________________________________

FRONTIER (mHC)

FRONTIER SEMINAR:

Manifold Constrained Hyper Connection. mHC

Deep Seek.

good performance with less cost. mHC paper.

Res Net #1 cited paper. 10 layers to 100.

Then hundreds of layers, exploding, dim gradient.

Layer - input token, vector, embed representation, model.

stack layers by same size.

- learning, residual connection.

------

add layers up - solves gradient problem.

RESIDUALS. 

EXAMPLE: 5 + 1

EXAMPLE : 5 + 2.

Need shape to match.

nxn N to N Mapping.

SINGLE CONNECTION to HYPER CONNECTION.

- hyper means connect across all tokens.

HYPER-CONNECTION, from SINGLE (residual) connection.

Need to solve MAPPING. 

"Stack Mapping" - could be attention layer, feed forward layer.

Matrix Multiplication - to get back 1 token.

Like a multi dimensional probability.

-

STATIC (neuron) in respect to INPUT - weights (stay same).

DYNAMIC (neuron) weights changes based on input. 

SIGMOID. Dynamic Mapping - LINEAR PROJECTION.

FLatten Frame - Matrix Multiplication (again)

Mix down to from large to small - for next layer.

STATIC MAPPING:

Back from small to large 

Go from 1 to n.

BROADCASTING.

layers - dependent on input.

each layer new input.

need to keep shape

linear layer transform practice

trainable parameters,

7 billion parameters.

RED background - parameter.

YELLOW background - calculation.

Mapping is dynamically determined.

### N x N mapping

- sideways Matrix Multiplication (again)

- like number of gumballs in a jar.

- then into sand, and electrified into shape.

"trainable weights" - static weights.

"dynamic mapping" = many more "weights"!

Dynamic Mapping.

-----------------------

from simple residual
into hyper connection
solve mapping
Static
and
Dynamic
now

ADD MANIFOLD CONSTRAINT.

8 x 8 matrix (64 dimension)

constrain gumballs in giant jar.

All numbers, residual, IDENTITY MATRIX!

SIGMOID - from multinumber input

reduce to normalized 1. (tricky)

Same process - different direction.

same process - many times.

numbers closer to 1.

WHY: project to lower sub dimensions.

focus in and out.

crumpled paper is like - manifold!

add to complete hyper connection.


________________________

mHC, TTS (Test to Speech)

MIT

First lecture ever on mHC. with T. Y from Colorado University.

FRONTIER LECTURE!

evergreen - mHC span the test of time.

Copious Notes.

Beginning of 10 week lecture series.

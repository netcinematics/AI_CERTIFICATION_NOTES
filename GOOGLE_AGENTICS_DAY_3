GOOGLE_AGENTICS_DAY3

Day 3 (Context Engineering: Sessions & Memory)
Welcome to Day 3.

This whitepaper explores context engineering as the 
practice of dynamically assembling and managing information within an 
agent's context window to create 
stateful and personalized Al experiences. It defines 
Sessions as the container for a single, 
immediate conversation's history, and 
Memory as the long-term persistence mechanism.

In the codelabs, you will learn how to make 
agents stateful by managing conversation history through 
context engineering in ADK, and working memory within a 
session, allowing your agent to remember 
context and have coherent, multi-turn conversations. In the second 
notebook, you'll give your agent 
long-term memory that persists across different sessions.

 whitepaper explores the critical role of 
Sessions and Memory in building 
stateful, intelligent LLM agents to empower developers to create more powerful, 
personalized, and persistent AI experiences. To enable Large Language Models 
(LLMs) to remember, learn, and personalize interactions, developers must 
dynamically assemble and manage information within their 
context windowâ€”a process known as Context Engineering.

1 context engineer (foundation)
  - stateless to statefulness
  - adapt to input, manage data
  - few-shot reasoning
  - context rot (dynamic history mutation)
  1. get memories
  2. block items
  3. session data
  4. update long term memory 

2 sessions interactions 
   - session object
   - mutability, summary 
   - MAS (multi-agent systems)
   - all write to same log.
   - blackbox isolation.
    - framework agnostic.


3 memory
   - workbench
  - filing cabnet
   - model armor
   - TTL scrub identity
   - compaction stradegy,
   - sliding window
  - recursive summary
   - keep essence, drop tkn count.
   -async background compaction


q n a
merging duplicates.
from amnesia to memory.
truncation
static instructions 
(non negotiable safety policy)
end user message (clean)
turn Instructions - dynamic steering 
dynamic CONTEXT 
context caching, store, inject not call llm
faster and cheaper.

memories stored in vector database.rag
many memory designs.
short term memory compaction,
how to carry over memory long term
episodic, semantic, procedure memory
add, retrieve, update, prune.
hybrid: keyword search, rankers, rerankers.
graph based db connectedness.
do not limit yourself w memz

mechanisms, of inconsistent user claim
maintain good memories:
extraction, consolidate - compaction.
signal in noise 
data provenance.
defer to higher trust data.
memory system, memory bank.
define exact memory to safe.
prompt injection (malicious query)
model armor - detects PI.
INFERENCE - callout memories
use this woth caution.

rag - retrieval relevance
metrics useful for different things
non retrieval - what is
injected into context?
actual solution?
collection of sub behaviors.
llm as a judge. rate this on requirements.
grounded metrics - faithfulness.
catch regression.


storytelling for memory
project state is evolving story
organized chronologically 
with timeline. fixation.
narrative in model mind.

putting context in context.

rjack?

jay alammar

quiz:

session memmory:
workbench file cabinet

context engineering:
dynamic assembly context window
context compaction
recursive summary

declarative or procedural:
knowing what
knowing how
facts events skills

memory consolidation
merge memories
de duplicy

rag differ from memory
librarian facts, vs assistant.





knowing what
knowing how
rememvers processed.
structured user profile
rolling summary 
contact card
knowledge graph - how is x relayed to y and z
memory scope - global , user
extract, consolidate 
relevance decat
provenance, confidence scores,
revance decays, when not used


asynchronously saved memory store
memory as a tool
agent manages, c
create memory
check memore
retrieval
blend scores,
relevance
recency,
importance
useful context
proactive - preload
reactive, gets data on trigger
INFERENCE,
store in llm
placement signal authority
user profile
separated from PROMPT 
testing
rigorous
precision recall
latency
under 200ms
task success
use llm judge across test xases
work ench compaction
file cabnet
memories
fact tecal to 
personalized assistant 
lear personalize

